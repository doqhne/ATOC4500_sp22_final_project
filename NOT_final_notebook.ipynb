{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325e7b1a-6752-4e68-94f0-66453835e768",
   "metadata": {},
   "source": [
    "## **ATOC4500 Data Science Lab: Final Project**\n",
    "## **Using rapid ice loss events to predict when CESM1 ensemble members go ice free**\n",
    "#### **Author: Daphne Quint, daqu2831@colorado.edu**\n",
    "#### **Last updated: April 14, 2022**\n",
    "\n",
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8109c09-44e4-4b1e-bce5-ffaf336a87a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84337a16-1c71-4058-a049-e9b290f3d3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import resample\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de63e6-6768-4cb0-95ae-d03739d866cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa58a44-a6e9-4066-b02c-2566128a69aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_year(data, member):\n",
    "    '''\n",
    "    Finds the year 1 member goes below 1 million square km\n",
    "    '''\n",
    "    \n",
    "    data_ = data.sel(member=member)\n",
    "    \n",
    "    year = 2020\n",
    "    for i in data_:\n",
    "        if i>1:\n",
    "            year += 1\n",
    "        else:\n",
    "            return year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f73e1f47-5d31-4ba3-94ca-cc08d2dcdee8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_df(month, model_name, num_mems):\n",
    "    # select correct data\n",
    "    if model_name == 'MPI_ESM1':\n",
    "        nb_ext_data = nb_ext_data_MPI\n",
    "        length_data = length_data_MPI\n",
    "    else:\n",
    "        nb_ext_data = nb_ext_data_CESM\n",
    "        length_data = length_data_CESM\n",
    "    \n",
    "    #### find ice free year for that month\n",
    "    \n",
    "    ###first find 5 year mean\n",
    "    \n",
    "    # define September SIE\n",
    "    SIE_sept = SIE[model_name].sel(time=SIE['time.month']==month).sel(member=np.arange(1, num_mems+1, 1))\n",
    "\n",
    "    # find the 5 year running mean\n",
    "    five_year_mean = SIE_sept*0\n",
    "\n",
    "    for i in range(1, num_mems+1):\n",
    "        five_year_mean[i-1] = SIE_sept.sel(member=i).rolling(time=5).mean()\n",
    "\n",
    "    five_year_mean = five_year_mean.sel(time=slice('2020', '2100'))\n",
    "    \n",
    "    ### then find the ice free year\n",
    "    ice_free_year = []\n",
    "    for i in range(1, num_mems+1):\n",
    "        ice_free_year.append(find_year(five_year_mean, i))\n",
    "    ice_free_year = np.array(ice_free_year)\n",
    "    \n",
    "    #### find max amt of ice lost in the month\n",
    "    \n",
    "    ice_lost_max = []\n",
    "    for i in range(1, num_mems+1):\n",
    "        ice_lost_max.append(float(nb_ext_data['RILE Indicator'].sel(member=i).sel(month=month).min().values))\n",
    "    ice_lost_max = np.array(ice_lost_max)\n",
    "    \n",
    "    #### find longest duration for the month\n",
    "    \n",
    "    length_max = []\n",
    "    for i in range(1, num_mems+1):\n",
    "        length_max.append(float(length_data['Length'].sel(member=i).sel(month=month).max().values))\n",
    "    length_max = np.array(length_max)\n",
    "    \n",
    "    #### create dataframe\n",
    "    \n",
    "    member = pd.DataFrame(data=np.arange(1, num_mems+1), columns=['Member'])\n",
    "    month = pd.DataFrame(data=(np.zeros(num_mems)+month), columns=['Month'])\n",
    "    ice_free_yr_df = pd.DataFrame(data=ice_free_year, columns=['Ice Free Year'])\n",
    "    ice_lost_max_df = pd.DataFrame(data=ice_lost_max, columns=['Max Ice Lost'])*-1\n",
    "    length_max_df = pd.DataFrame(data=length_max, columns=['Longest Duration'])\n",
    "    \n",
    "    this_month_df = pd.concat([member, month, ice_free_yr_df, ice_lost_max_df, length_max_df], axis=1)\n",
    "    \n",
    "    return this_month_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1249db24-92b9-47c4-b717-dc994b83f24e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def define_holdout_data(x, y, verbose):\n",
    "    \"\"\"Perform a 80/20 test-train split (80% of data is training, 20% is testing). Split is randomized with each call.\"\"\"\n",
    "    random_state = randint(0,1000)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=random_state)\n",
    "    if verbose==True:\n",
    "        print(\"Prior to scaling and rebalacing...\")\n",
    "        print(\"Shape of training predictors: \"+str(np.shape(x_train)))\n",
    "        print(\"Shape of testing predictors: \"+str(np.shape(x_test)))\n",
    "        print(\"Shape of training predictands: \"+str(np.shape(y_train)))\n",
    "        print(\"Shape of testing predictands: \"+str(np.shape(y_test)))\n",
    "        print(\" \")\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011bcf47-8a5d-4439-a2da-c0b2d43c1773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scale_data(x_train, x_test):\n",
    "    \"\"\"\n",
    "    Scale training data so that model reaches optimized weights much faster. \n",
    "    \n",
    "    *All data that enters the model should use the same scaling used to scale the training data.*\n",
    "    Thus, we also perform scaling on testing data for validation later. \n",
    "    Additionally, we return the scaler used to scale any other future input data.\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = preprocessing.MinMaxScaler() # normalize \n",
    "    x_train_scaled = pd.DataFrame(data=scaler.fit_transform(x_train),index=x_train.index,columns=x_train.columns) \n",
    "    x_test_scaled = pd.DataFrame(data=scaler.transform(x_test),index=x_test.index,columns=x_test.columns)\n",
    "    \n",
    "    return scaler, x_train_scaled, x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b7fe30-eaaa-4297-a035-496d8012e1d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance_data(x,y,verbose):\n",
    "    \"\"\"Resample data ensure model is not biased towards a particular outcome of precip or no precip.\"\"\"\n",
    "    # Combine again to one dataframe to ensure both the predictor and predictand are resampled from the same \n",
    "    # observations based on predictand outcomes. \n",
    "    dataset = pd.concat([x, y],axis=1)\n",
    "\n",
    "    # Separating classes\n",
    "    early = dataset[dataset['early_bin'] == 1]\n",
    "    not_early = dataset[dataset['early_bin'] == 0]\n",
    "\n",
    "    random_state = randint(0,1000)\n",
    "    oversample = resample(early, \n",
    "                           replace=True, \n",
    "                           n_samples=len(not_early), #set the number of samples to equal the number of the majority class\n",
    "                           random_state=random_state)\n",
    "\n",
    "    # Returning to new training set\n",
    "    oversample_dataset = pd.concat([not_early, oversample])\n",
    "\n",
    "    # reseparate oversampled data into X and y sets\n",
    "    x_bal = oversample_dataset.drop(['early_bin'], axis=1)\n",
    "    y_bal = oversample_dataset['early_bin']\n",
    "\n",
    "    if verbose==True:\n",
    "        print(\"After scaling and rebalacing...\")\n",
    "        print(\"Shape of predictors: \"+str(np.shape(x_bal)))\n",
    "        print(\"Shape of predictands: \"+str(np.shape(y_bal)))\n",
    "        print(\" \")\n",
    "    \n",
    "    return x_bal, y_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c6c7ac5-6653-41ea-bbc7-b5ea288fa088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataprep_pipeline(x, y, verbose):\n",
    "    \"\"\" Combines all the functions defined above so that the user only has to \n",
    "    call one function to do all data pre-processing. \"\"\"\n",
    "    # verbose=True prints the shapes of input & output data\n",
    "\n",
    "    # split into training & testing data\n",
    "    x_train, x_test, y_train, y_test = define_holdout_data(x, y, verbose) \n",
    "\n",
    "    # perform feature scaling\n",
    "    scaler, x_train_scaled, x_test_scaled = scale_data(x_train, x_test)\n",
    "\n",
    "    # rebalance according to outcomes (i.e., the number of precipitating \n",
    "    # observations & non-precipitating outcomes should be equal)\n",
    "    if verbose==True:\n",
    "        print(\"for training data... \")\n",
    "    x_train_bal, y_train_bal = balance_data(x_train_scaled, y_train, verbose)\n",
    "    if verbose==True:\n",
    "        print(\"for testing data... \")\n",
    "    x_test_bal, y_test_bal = balance_data(x_test_scaled, y_test, verbose)\n",
    "    \n",
    "    return x_train_bal, y_train_bal, x_test_bal, y_test_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21cadbdb-d86f-449d-8953-899ec4b88edf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bin_metrics(x, y):\n",
    "    \"\"\"Prints accuracy and recall metrics for evaluating \n",
    "    classification predictions.\"\"\"\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(x, y)\n",
    "    recall = metrics.recall_score(x, y)\n",
    "\n",
    "    print('Accuracy:', round(accuracy, 4))\n",
    "    print('Recall:', round(recall, 4))\n",
    "    \n",
    "    return accuracy, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162157bf-c037-454f-9b3c-6c611247812b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_cm(x, y):\n",
    "    \"\"\"Plots the confusion matrix to visualize true \n",
    "    & false positives & negatives\"\"\"\n",
    "    cm = confusion_matrix(x, y)\n",
    "    df_cm = pd.DataFrame(cm, columns=np.unique(x), index = np.unique(x))\n",
    "    df_cm.index.name = 'Actual'\n",
    "    df_cm.columns.name = 'Predicted'\n",
    "    sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 25}, fmt='g')# font size\n",
    "    plt.ylim([0, 2])\n",
    "    plt.xticks([0.5, 1.5], ['Negatives','Positives'])\n",
    "    plt.yticks([0.5, 1.5], ['Negatives','Positives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccc03525-4704-476a-8770-2f216fd0855c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rand_atmos_conditions_precip(index='rand'):\n",
    "    \"\"\"\n",
    "    Function returns atmospheric conditions in a dataframe as well as the scaled\n",
    "    conditions in a numpy array so that they output a prediction in the model.\n",
    "    \n",
    "    If no input is passed, the function will randomly generate an in index to \n",
    "    choose from those observations in some training data with precipitation. \n",
    "    Otherwise, an integer index between 0 and 200 should be passed.\n",
    "    \"\"\"\n",
    "    # First, perform a test-train split\n",
    "    x_train, x_test, y_train, _ = define_holdout_data(x, y, verbose=False) \n",
    "\n",
    "    # perform feature scaling\n",
    "    _, x_train_scaled, _ = scale_data(x_train, x_test)\n",
    "\n",
    "    # this is what will go into the model to output a prediction\n",
    "    if index=='rand':\n",
    "        index = randint(0,len(y_train[y_train==1].index)) \n",
    "    precipindex = y_train[y_train==1].index.values[index]\n",
    "    testpredictor = x_train_scaled.loc[precipindex] \n",
    "    \n",
    "    return sept_df.iloc[precipindex], testpredictor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a7517-e004-491c-b0fc-c9e7c4322903",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f823bd6-5e8d-40c2-8fd9-ad533dade7a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/home/daphne/Documents/School/research/icefreeproject/Data/RILE_nbext_CESM.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/lru_cache.py:53\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/home/daphne/Documents/School/research/icefreeproject/Data/RILE_nbext_CESM.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/daphne/Documents/School/research/icefreeproject/Data/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Amount of sea ice lost and Sea ice extent data for each RILE\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m nb_ext_data_CESM \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRILE_nbext_CESM.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m nb_ext_data_MPI \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(data_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRILE_nbext_MPI.nc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# length data (consecutive years in a row there is a rile for that month)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/api.py:495\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    484\u001b[0m     decode_cf,\n\u001b[1;32m    485\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    492\u001b[0m )\n\u001b[1;32m    494\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 495\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    502\u001b[0m     backend_ds,\n\u001b[1;32m    503\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m )\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:553\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    534\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    550\u001b[0m ):\n\u001b[1;32m    552\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 553\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    565\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:382\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    376\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    377\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    378\u001b[0m )\n\u001b[1;32m    379\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    380\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    381\u001b[0m )\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:330\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:385\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    386\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/file_manager.py:187\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xarray/backends/file_manager.py:205\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    203\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    204\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 205\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overriden when opened again\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2307\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:1925\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/home/daphne/Documents/School/research/icefreeproject/Data/RILE_nbext_CESM.nc'"
     ]
    }
   ],
   "source": [
    "data_path = '/home/daphne/Documents/School/research/icefreeproject/Data/'\n",
    "\n",
    "# Amount of sea ice lost and Sea ice extent data for each RILE\n",
    "nb_ext_data_CESM = xr.open_dataset(data_path+'RILE_nbext_CESM.nc')\n",
    "nb_ext_data_MPI = xr.open_dataset(data_path+'RILE_nbext_MPI.nc')\n",
    "\n",
    "# length data (consecutive years in a row there is a rile for that month)\n",
    "length_data_CESM = xr.open_dataset(data_path+'CESM_rile_length.nc')\n",
    "length_data_MPI = xr.open_dataset(data_path+'MPI_rile_length.nc')\n",
    "\n",
    "# extent data - can be used to find ice free year for each member\n",
    "SIE = xr.open_dataset(data_path+'CLIVAR_SIE_1850_2100_RCP85.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd5656f-c160-41d7-aba1-48bdbb02251e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Munge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f49110-ee7d-46b7-8c26-253dc3cc25fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for CESM\n",
    "CESM_sept_df = create_df(9, 'CESM1', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e3a12e-d557-4161-8251-f5fa59c6674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for MPI\n",
    "MPI_sept_df = create_df(9, 'MPI_ESM1', 100)\n",
    "\n",
    "# add 40 to MPI members so they are distinct from CESM members\n",
    "MPI_sept_df.Member = MPI_sept_df.Member + 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde032b-33d1-4488-8619-85fd2fb233ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the two dataframes to create a 'megamodel'\n",
    "sept_df = pd.concat([CESM_sept_df, MPI_sept_df], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95176e4f-a4d4-4204-87c8-0a412f046566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if there are nan values\n",
    "print(np.any(np.isnan(sept_df)))\n",
    "\n",
    "# drop nan values so they don't mess up the data science method\n",
    "sept_df = sept_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111a806-263b-4fa4-bfd4-6d796d8eae6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Apply Data Science Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5459f-fb89-4b52-85a5-91627026cb80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992853b0-9162-4090-9e67-a7e71d181b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a year to separate the ice free column into 2 groups\n",
    "ify_mean = sept_df['Ice Free Year'].mean()\n",
    "ify_std  = sept_df['Ice Free Year'].std()\n",
    "split_yr = ify_mean - (1.5*ify_std)\n",
    "print(split_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d324a2-10f6-4c5b-81ed-d3cea1f92286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature that indicates whether or not the member goes ice free before 2046\n",
    "sept_df['early_bin'] = np.array(sept_df['Ice Free Year']<=2046).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b9d1d7-f0f1-49c0-b093-d1c1874b177e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features that we will use to predict ice free year\n",
    "x = sept_df.drop(['Month','Member', 'Ice Free Year', 'early_bin'],axis=1)\n",
    "\n",
    "# what we are trying to predict- early ice free year\n",
    "y = sept_df.drop(['Month','Member', 'Longest Duration', 'Max Ice Lost', 'Ice Free Year'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea640308-3e62-407f-b4fa-c8e8a76d61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if the results are balanced\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a924f-fef1-41ff-a1ce-0549bac58f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a test-train split, perform feature scaling, and the rebalance our dataset\n",
    "x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3bdbe1-5f85-4bb2-a0a0-4ff691027655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the logistic regression model\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# fit the model to scaled & balanced training data\n",
    "lr.fit(x_train_bal, y_train_bal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2394b408-0275-4505-b0b0-948f243a1d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the test data - data not used for the logistic regression - to see how well the model does\n",
    "y_pred = lr.predict(x_test_bal)\n",
    "\n",
    "plot_cm(y_test_bal, y_pred);\n",
    "lr_acc, lr_rec = bin_metrics(y_test_bal, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d224a3b-f097-4e9c-b10b-2b9012a47cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if the the logistic regression is overfitting or underfitting\n",
    "\n",
    "# Compare testing data metrics to data training metrics\n",
    "print(\"Training metrics:\")\n",
    "pred_train= lr.predict(x_train_bal) \n",
    "bin_metrics(y_train_bal,pred_train);\n",
    "\n",
    "# As a reminder, display testing metrics\n",
    "print(\" \")\n",
    "print(\"Testing metrics:\")\n",
    "bin_metrics(y_test_bal, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91ba52-dddc-4697-a686-4fcc7cae0f32",
   "metadata": {},
   "source": [
    "Since testing metric is slightly less than the training metric, we may be overfitting, i.e, the model is too complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b9850-2afe-4b3d-aab2-b7884a35d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the Logistic Regression model\n",
    "    # randomly choose some conditions using the rand_atmos_conditions function\n",
    "origvals, testpredictor = rand_atmos_conditions_precip()\n",
    "\n",
    "print(origvals) # observation from original dataframe\n",
    "print(\"\")\n",
    "print(testpredictor) # scaled observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26305abf-1f95-4963-bc3a-70967a91878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_prediction = lr.predict_proba(np.array(testpredictor).reshape(1, -1))[0][1]*100 \n",
    "print(\"The conditions are: \")\n",
    "print(origvals)\n",
    "print(\" \")\n",
    "print(\"There is a {0:.{digits}f}% chance a member will go ice free in 2046 or before given those conditions.\".format(lr_prediction, digits=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381aa40-68af-4e6b-8775-8d5a4c02635f",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb35fef-7022-4d46-86cf-3c9595041e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a test-train split, perform feature scaling, and the rebalance our dataset\n",
    "x_train_bal, y_train_bal, x_test_bal, y_test_bal = dataprep_pipeline(x, y, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3918d0-c833-4ece-8ba3-e3d253e9f689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and tune the random forest model\n",
    "\n",
    "# choose the hyperparameters\n",
    "    # choose to adjust the number of estimators in the forest and the depth of the tree\n",
    "    \n",
    "acc_scores = []\n",
    "rec_scores = []\n",
    "\n",
    "num_est = [10, 50, 500] # number of trees\n",
    "depth = [2, 10, 100] # number of decisions\n",
    "for i in num_est:\n",
    "    start = time.time()\n",
    "    print(\"Number of estimators is \"+str(i))\n",
    "\n",
    "    for k in depth:\n",
    "        print(\"depth is \"+str(k))\n",
    "        forest = RandomForestClassifier(n_estimators=i, max_depth=k)\n",
    "        forest.fit(x_train_bal, y_train_bal)\n",
    "        \n",
    "        # cross validate & evaluate metrics based on testing data\n",
    "        pred_test= forest.predict(x_test_bal)\n",
    "        acc_val = metrics.accuracy_score(y_test_bal, pred_test)\n",
    "        acc_scores.append(acc_val)\n",
    "        rec_val = metrics.recall_score(y_test_bal, pred_test)\n",
    "        rec_scores.append(rec_val)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Random Forest took \"+str(end-start)+\" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5fbee7-3704-4af2-b190-bf664e0e34e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acc_scores, marker='o', color='black')\n",
    "plt.plot(rec_scores, marker='o', color='blue')\n",
    "print(\"Max Accuracy (black):\", round(max(acc_scores), 4))\n",
    "print(\"Max Recall (blue):\", round(max(rec_scores), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebd8a4-ae40-461a-874e-447bd6c7bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameter selection - choose x=0 for maximum accuracy and recall\n",
    "forest = RandomForestClassifier(n_estimators=50, max_depth=2);\n",
    "forest.fit(x_train_bal, y_train_bal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d744ed6-90c5-4721-a188-a620004bca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess the Random Forest's performance using testing data\n",
    "pred_test= forest.predict(x_test_bal)\n",
    "\n",
    "# Calculate metrics & plot a confusion matrix based on how well model simulates testing data\n",
    "forest_acc, forest_rec = bin_metrics(y_test_bal, pred_test)\n",
    "\n",
    "#plot\n",
    "plot_cm(y_test_bal, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a27083-e8b4-42e2-b2cf-2f8615651e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see if the Random Forest is overfitting\n",
    "\n",
    "# Compare testing data metrics to data training metrics.\n",
    "print(\"Training metrics:\")\n",
    "rf_pred_train= forest.predict(x_train_bal) \n",
    "bin_metrics(y_train_bal,rf_pred_train);\n",
    "\n",
    "# display testing metrics:\n",
    "print(\" \")\n",
    "print(\"Testing metrics:\")\n",
    "bin_metrics(y_test_bal, pred_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6595f6e-04ba-47fd-ac5d-8bd56d7aa889",
   "metadata": {},
   "source": [
    "Testing accuracy is slightly less than the training data, while the recall is greater which may indicate overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d06f6-58f3-4128-aa9a-5670bd064913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with the Random Forest\n",
    "\n",
    "# prediction output is in the format [probability not before 2046, probability before 2046]\n",
    "forest_prediction = forest.predict_proba(np.array(testpredictor).reshape(1, -1))[0][1]*100 \n",
    "print(\"The conditions are: \")\n",
    "print(origvals)\n",
    "print(\" \")\n",
    "print(\"There is a {0:.{digits}f}% chance that a member will go ice free in 2046 or before given those conditions.\".format(forest_prediction, digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461b36cd-88ed-4d76-b551-1a04066510a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = pd.DataFrame({'Metrics':['Accuracy','Recall','Prediction example'],\n",
    "     'Logistic Regression':[lr_acc, lr_rec, lr_prediction],\n",
    "    'Random Forest':[forest_acc, forest_rec, forest_prediction]})\n",
    "model_metrics = model_metrics.set_index('Metrics')\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5a314-933a-4662-b6d5-e8f44638f69f",
   "metadata": {},
   "source": [
    "## Step 4: Present graphs visually using 2-3 graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa6437-4d78-4036-9a56-a46d9bd029d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0b4504a-3424-4726-8e41-92f1b69220d7",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c02323-efb1-4880-abcc-06e5db040547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
